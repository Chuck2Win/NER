{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_BERT_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPAxGFa6nc03NTh8b2bKuD2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chuck2Win/NER/blob/main/NER_BERT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOlYOJpS0GDg",
        "outputId": "eb3bcb5a-9514-4e46-8a60-3934ca05ab3e"
      },
      "source": [
        "! pip install transformers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import os\n",
        "os.chdir('./gdrive/My Drive/ner')\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 33.9MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.0)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.6.1\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouMDX7jdw2S_"
      },
      "source": [
        "class preprocessing(object):\n",
        "    def __init__(self, max_length = 64):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased',do_lower_case = False)\n",
        "        self.label2idx = {i:_ for _,i in enumerate([\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"])}\n",
        "        self.max_length = max_length    \n",
        "    \n",
        "    def get_labels(self):\n",
        "        return self.label2idx\n",
        " \n",
        "    def read_file(self):\n",
        "        f = open('./train.txt','r')\n",
        "        tagged_sentences = []\n",
        "        sentences = []\n",
        "        for i in f:\n",
        "            if i.startswith('-DOCSTART') or i=='\\n': \n",
        "                if len(sentences)>0:\n",
        "                    tagged_sentences.append(sentences)\n",
        "                    sentences = []\n",
        "                continue\n",
        "            x=i.split()\n",
        "            x = [re.sub('\\n','',j) for j in x] # 단어, 품사태그, 청크 태크, 개체명 태크\n",
        "            sentences.append([x[0],x[-1]]) # 단어, 개체명 태그\n",
        "        ner_tags=[]\n",
        "        words = []\n",
        "        for sentence in tagged_sentences:\n",
        "            word, tag = zip(*sentence)\n",
        "            words.append(list(word))\n",
        "            ner_tags.append(list(tag))\n",
        "        #return words, ner_tags\n",
        "        # ner tag를 idx로 변환\n",
        "        idx_tags = []\n",
        "        for i in ner_tags:\n",
        "            t = []\n",
        "            for j in i:\n",
        "                t.append(self.label2idx[j])\n",
        "            idx_tags.append(t)\n",
        "        # return idx_tags\n",
        "        self.words = words\n",
        "        self.ner_tags = idx_tags\n",
        "        # sub word token화 시키고, ner tag를 extend시킴('X'추가)\n",
        "        tokenized_words = []\n",
        "        extend_ner_tags = []\n",
        "        subword_first_tags = []\n",
        "        for sentence,ner_tag in zip(self.words, self.ner_tags):\n",
        "            t = []\n",
        "            ts = []\n",
        "            sf = []\n",
        "            for word,tag in zip(sentence,ner_tag):\n",
        "                o = self.tokenizer.tokenize(word)\n",
        "                t.extend(o)\n",
        "                ts.extend([tag]*(len(o))) \n",
        "                sf.extend([1]+[0]*(len(o)-1))\n",
        "            tokenized_words.append(t)\n",
        "            extend_ner_tags.append(ts)\n",
        "            subword_first_tags.append(sf)\n",
        "        self.data = pd.DataFrame()\n",
        "        self.data['words'] = tokenized_words\n",
        "        self.data['labels'] = extend_ner_tags\n",
        "        self.data['labels_mask'] = subword_first_tags\n",
        "        # truncation\n",
        "        self.data['labels'] = self.data['labels'].apply(lambda i : i[:self.max_length-2]) \n",
        "        self.data['labels_mask'] = self.data['labels_mask'].apply(lambda i : i[:self.max_length-2]) # first subword만 1이고 나머진 0\n",
        "        # pad\n",
        "        self.data['labels'] = self.data['labels'].apply(lambda i : [-1]+i+[-1]*(self.max_length-len(i)-1)) # [CLS],[SEP],[PAD]의 위치에는 -1을 기록\n",
        "        self.data['labels_mask'] = self.data['labels_mask'].apply(lambda i : [0]+i+[0]*(self.max_length-len(i)-1)) # first subword만 1 나머진 0\n",
        "        self.data['ids'] = self.data['words'].apply(lambda  i : self.tokenizer.encode(i,padding = 'max_length', max_length = self.max_length, truncation = True))\n",
        "\n",
        "      # assert np.array(self.data.tags.tolist()).shape[1]==self.max_length\n",
        "    \n",
        "    def make_data_loader(self, batch_size = 32):\n",
        "        # ids\n",
        "        # attention mask\n",
        "        # segment ids\n",
        "        # labels\n",
        "        \n",
        "        ids = torch.LongTensor(self.data.ids.tolist())\n",
        "        # bert model에선 mask할 곳이 False 안할 곳이 True\n",
        "        attention_masks = ids.eq(self.tokenizer.pad_token_id)\n",
        "        attention_masks = (attention_masks==False).long() \n",
        "        # segment가 다 0이므로\n",
        "        token_type_ids = torch.zeros_like(ids)\n",
        "\n",
        "        # label 관련\n",
        "        labels = torch.LongTensor(self.data.labels.tolist())\n",
        "        labels_mask = torch.LongTensor(self.data.labels_mask.tolist())\n",
        "        \n",
        "        dataset = TensorDataset(ids,attention_masks,token_type_ids,labels,labels_mask)\n",
        "        data_loader = DataLoader(dataset,batch_size = batch_size, shuffle = True)\n",
        "        return data_loader"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t57KXBarzbKn"
      },
      "source": [
        "p = preprocessing()\n",
        "p.read_file()\n",
        "data = p.data\n",
        "data_loader = p.make_data_loader()"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oVjhuRghqoJ"
      },
      "source": [
        "batch = next(iter(data_loader))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "7a_gb3KKjpca",
        "outputId": "a22a7cd1-9271-4259-b2c7-fccb2c894f2f"
      },
      "source": [
        "batch = batch.to('cuda')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-28d115d0f786>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJWLw_cjhvF5"
      },
      "source": [
        "l=batch['labels']\n",
        "m=batch['labels_mask']"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XIRdYBc5gsy"
      },
      "source": [
        "# Bert token classification \n",
        "pooling layer를 통과하지 않은 final hidden layer를 사용할 생각임.  \n",
        "물론 실제 논문에서는 마지막 4개 layer를 concat한 경우가 가장 성능이 좋았음.(Feature based approach에서)  \n",
        "나는 Fine tunning 방식으로 하고, Last hidden layer만을 활용할 것이다.(pooling layer를 통과하지 않고)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3VgSvD75-0J"
      },
      "source": [
        "class my_model(nn.Module):\n",
        "    def __init__(self,bert,n_labels):\n",
        "        super().__init__()\n",
        "        self.bert = bert\n",
        "        self.linear = nn.Linear(768,n_labels)\n",
        "        self.droput = nn.Dropout(0.1)\n",
        "    def forward(self, ids, attention_mask, token_type_ids):\n",
        "        output = self.bert.forward(input_ids = ids, attention_mask = attention_mask, token_type_ids= token_type_ids)\n",
        "        output = output.last_hidden_state\n",
        "        out = self.linear.forward(self.droput.forward((output)))\n",
        "        return out        "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1hK1mqK7eZy",
        "outputId": "c429819e-9ba1-47b7-cc2f-920fe001fe25"
      },
      "source": [
        "epochs  = 10\n",
        "bert = BertModel.from_pretrained('bert-base-cased',add_pooling_layer = False)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = my_model(bert,9).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr= 1e-5)\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDlkvBwe7kEB",
        "outputId": "e755e6be-d304-4501-de9d-4e222772531b"
      },
      "source": [
        "for epoch in tqdm(range(1,epochs+1),mininterval=60):\n",
        "    model.train()\n",
        "    check = []\n",
        "    total_loss = 0\n",
        "    predicted = []\n",
        "    actual = []\n",
        "    l = 0\n",
        "    for data in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = tuple(i.to(device) for i in data)\n",
        "        out = model.forward(data[0],data[1],data[2])\n",
        "        out = out[data[-1]==1]\n",
        "        labels = data[3][data[-1]==1]\n",
        "        l+=len(labels)\n",
        "        loss = criterion(out,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        predicted.extend(out.argmax(-1).reshape(-1).cpu().tolist())\n",
        "        actual.extend(labels.reshape(-1).cpu().tolist())\n",
        "        check.append(loss.item())\n",
        "        total_loss+=loss.item()\n",
        "    total_loss=total_loss/l\n",
        "    # print(total_loss)\n",
        "    if epoch % 5==0:\n",
        "        print(total_loss)\n",
        "        print(classification_report(actual,predicted))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [05:53<08:50, 88.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.013311877141907425\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 50%|█████     | 5/10 [07:22<07:22, 88.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    169286\n",
            "           1       0.97      0.97      0.97      3435\n",
            "           2       0.96      0.96      0.96      1155\n",
            "           3       0.99      0.99      0.99      6580\n",
            "           4       0.99      1.00      1.00      4506\n",
            "           5       0.98      0.98      0.98      6312\n",
            "           6       0.98      0.99      0.99      3697\n",
            "           7       0.99      0.99      0.99      7132\n",
            "           8       0.99      0.98      0.98      1157\n",
            "\n",
            "    accuracy                           1.00    203260\n",
            "   macro avg       0.98      0.98      0.98    203260\n",
            "weighted avg       1.00      1.00      1.00    203260\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [13:15<01:28, 88.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.004075519219048202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [14:44<00:00, 88.45s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    169286\n",
            "           1       0.99      0.99      0.99      3435\n",
            "           2       0.99      0.99      0.99      1155\n",
            "           3       1.00      1.00      1.00      6580\n",
            "           4       1.00      1.00      1.00      4506\n",
            "           5       0.99      0.99      0.99      6312\n",
            "           6       1.00      1.00      1.00      3697\n",
            "           7       1.00      1.00      1.00      7132\n",
            "           8       1.00      1.00      1.00      1157\n",
            "\n",
            "    accuracy                           1.00    203260\n",
            "   macro avg       0.99      0.99      0.99    203260\n",
            "weighted avg       1.00      1.00      1.00    203260\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2-Uphi2xZFn"
      },
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predicted = []\n",
        "    actual = []\n",
        "    for data in data_loader:\n",
        "        #optimizer.zero_grad()\n",
        "        input_ids = data['input_ids'].to('cuda')\n",
        "        attention_mask = data['attention_mask'].to('cuda')\n",
        "        token_type_ids = data['segment_ids'].to('cuda')\n",
        "        labels = data['labels'].to('cuda')\n",
        "        out = model.forward(input_ids,attention_mask, token_type_ids)\n",
        "        loss = criterion(out.transpose(1,2),labels)\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "        predicted.extend(out.argmax(-1).reshape(-1).cpu().tolist())\n",
        "        actual.extend(labels.reshape(-1).cpu().tolist())\n",
        "        #check.append(loss.item())\n",
        "        #total_loss+=loss.item()"
      ],
      "execution_count": 121,
      "outputs": []
    }
  ]
}